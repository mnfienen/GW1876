{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"AW&H2015.png\" style=\"float: right\">\n",
    "\n",
    "<img src=\"flopylogo.png\" style=\"float: left\">\n",
    "\n",
    "<img src=\"PEST++V3_cover.jpeg\" style=\"float: center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With great power comes great responsibility:  Addressing the ill-posed highly parameterized problem with our second line of defense Tikhonov regularization\n",
    "\n",
    "We are now moving up the simplicity-complexity curve because now we have many more parameters, even more than our observations.  Have we passed the sweetspot?  \n",
    "\n",
    "<img src=\"Hunt1998_sweetspot.png\" style=\"float: center\">\n",
    "\n",
    "\n",
    "Again, it is not simply the number of parameters that is at issue.  The better way to think of it is that we just want to avoid \"living beyond our means\". That is, we do not bring more parameters to bear than we have the ability to constrain.  We constrain them observations as we have seen so far, but we also know things about the system that are not hard data like measurements.  This \"soft-knowledge\" can also be applied to  constrain our parameters through a mechanism called __\"Tikhonov regularization\"__.  In this formulation of the inverse problem, we add a second term to our \"best fit\" metric Phi. This second term reflects the deviation from our soft-knowledge of the system, and is a penalty to our fit. Here's how it looks using the Anderson et al. (2015) forumlation:\n",
    "\n",
    "<img src=\"tik-reg_eq9.8.png\" style=\"float: center\">\n",
    "\n",
    "\n",
    "As first term after the equal sign is our __measurement objective function__, which we've been working with all week.  The last term on the right is called the __\"regularization objective function\"__. These 2 terms combine to create a __total Phi__ on the left.  __Now Total Phi is what we minimize__, which means we are minimizing our observed-to-simulated residuals __AND__ the deviation from soft-knowledge.  So in this way Tikhonov regularization is a \"dual-constrained minimization\".  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Andrey Tikhonov\n",
    "<img src=\"Andrey_Tikhonov_picture.jpeg\" style=\"float: center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anderson et al. (2015) looks a little closer at this in equation 9.9:\n",
    "\n",
    "\n",
    "<img src=\"tik-reg_eq9.9.png\" style=\"float: center\">\n",
    "\n",
    "The first term to the right of the equals sign is the measurement objective function from\n",
    "Eqn (9.6), which is calculated as the sum of squared weighted residuals, where *n* residuals,\n",
    "*ri*, are calculated from hard knowledge and wi are their respective weights. The second\n",
    "term quantifies the penalty resulting from deviations from soft knowledge as the sum\n",
    "of *q* deviations from *j* soft knowledge conditions *fj*, where *fj* is a function of model parameters\n",
    "*p*. A calibrated model, therefore, is found by minimizing both the measurement\n",
    "objective function (hard data) and the soft knowledge penalty.\n",
    "\n",
    "### Take-home point #1 from these equations: \n",
    "\n",
    "When Tikhonov is set up correctly, PEST should only deviate from the preferred condition when there is a suffient improvement in our fit to the observations (= the measurement objective function).  \n",
    "\n",
    "### Take-home point #2 from these equations:  \n",
    "\n",
    "The two contributors to our total phi are __*carried separately in the parameter estimation*__.  This is important:  in olden days only one objective function was used; deviations from \"prior information\" - i.e, the preferred conditions - directly penalized the *measurement* objective function.  However, when something is lumped it cannot be split.  With one combined ojbective function the modeler could not track what the parameter estimation was responding to - the change in fit or the change in adherence to preferred conditions? And it was difficult to know how to *a priori* weight deviations from preferred conditions appropriately so that they were seen in the parameter estimation but did not drown out the information in the observations.  The dual constrained optimization of Tikhonov regularization obivates these problems. So although our regularization information is found in the ``prior_information`` section In the PEST control file we append the letters \"regul\" to the parameter group name to let PEST know we are using this superior Tikhonov regularization form of prior information. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do we express soft-knowledge quantiatively so we can minimize it?\n",
    "\n",
    "We add preferred conditions.  These are typically:\n",
    "\n",
    "### 1) *preferred  value* - \"I believe this Kh parameter is around 1 m/d\"\n",
    "\n",
    "### 2) *preferred difference* - \"I believe  this area has a Kh 10 m/d higher than that area\" \n",
    "\n",
    "One of the most useful preferred condition for collapsing all these parameters to fewer bins is a special case of preferred difference where the difference = 0.  This is often called: __\"preferred homogeneity\"__ -  which equates to something along the lines of \"I believe this area has homogeneous Kh\" \n",
    "\n",
    "Of these, __preferred value__ is the easiest implement, and least memory intensive, preferred condition. Simply run the PEST utility _addreg1.exe_ on your PEST control file.  pyemu has similar functionality called \"*__zero_order_tikhonov__*\". But make sure the initial values represent your soft-knowledge!\n",
    "\n",
    "In pyemu also has preferred difference available - look for *\"__first_order_pearson_tikhonov__\"*.  We'll see both of these in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pilot point regularization can be propogated to other pilot points, or not.\n",
    "\n",
    "Here are two examples from Anderson et al. (2015).  For \"preferred value\" __(below (a), left)__ there is no cross-talk between pilot points.  The initial parameter value of each pilot point is the preferred value.  For preferred difference __(below (a), right)__, there is a radius of influence that connects the pilot point regularization (think correlation length from geostatistics).  \n",
    "\n",
    "\n",
    "<img src=\"Fig9.15a_Muffles_pp.png\" style=\"float: center\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Likewise, pilot-point regularization can also be grouped or limited to specific areas.  For example, if the geology of a site suggests distinct units you can only apply the preferred difference to just the zone:\n",
    "\n",
    "<img src=\"Fig9.15b_Kyle_Larry_pp.png\" style=\"float: center\">\n",
    "\n",
    "\n",
    "Here's the caption from Anderson et al. (2015) for posterity:  Figure 9.15 Pilot Points. (a) Network of pilot points in a watershed-scale groundwater flow model (left); linkages between pilot points (right) used to calculate Tikhonov regularization constraints for preferred homogeneity (modified from Muffels, 2008). (b) Network of pilot points used to represent two hydraulic conductivity zones where Tikhonov regularization is applied to pilot points within the same zone (modified from Davis and Putnam, 2013)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## But there is more to think about:\n",
    "\n",
    "Just like our observations, our preferred conditions are given a weight.  Typically it is uniform (usually 1) - this is what the PEST utility *addreg1.exe* does. On top of this, typically we have the regularization objective function set up to adjust the weights of the different parameter groups during the course of the parameter estimation (IREGADJ variable = 1 in the PEST control file).  See pages 17, 20, and page 34 of SIR 2010-5169. \n",
    "\n",
    "### But this is critical - these typically end up having somewhat subtle effects; the final say in trade-off between the measurement objective function and the regularization objective function is in a *user specified variable* in the PEST control file called:\n",
    "\n",
    "### PHIMLIM\n",
    "\n",
    "Many people missed the importance of this variable in the original Doherty (2003) paper that first showed PEST's pilot points and Tikhonov capabilities. This missed importance was addressed in detail in Fienen et al. (2009).  So, for you to do good modeling with these approaches it is critically important that you take this away, so we will state it again in bigger font:  \n",
    "\n",
    "# The final say in trade-off between the measurement objective function and the regularization objective function is in a *user specified variable* in the PEST control file called:\n",
    "\n",
    "# PHIMLIM\n",
    "\n",
    "PHIMLIM is the \"Target Measurement Objective Function\", which means rather than finding the best fit to the observations, PEST will hit this new PHIMLIM level and  *then find the minimum of the regularization objective function* (find the parameters that most closely match the preferred conditions while still keeping the PHIMLIM target measurement objective function). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A good way to think of this is that PHIMLIM controls the trade-off between the two parts of the righthand side of the equal sign in equation 9.8 above. We can plot this tradeoff as a Pareto front between adhereing to soft-knowledge (regularization objective function) and getting a better fit (measurement objective function). That looks like:\n",
    "\n",
    "\n",
    "<img src=\"Fig9.17_fit_vs_softknowledge_Pareto.png\" style=\"float: center\">\n",
    "\n",
    "\n",
    "## A key point is that many points on this curve could be considered a \"calibrated model\", which equals good fit and reasonable parameters. Which of these we choose is based on professional judgement.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final point:  Here's how PHIMLIM expresses itself on the optimal parameters look like this:\n",
    "\n",
    "<img src=\"Fig9.16_PHIMLIM.png\" style=\"float: center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So setting PHIMLIM is our primary way to control the degree of fitting, and keep us from *overfitting*\n",
    "\n",
    "# The suggested workflow is to:\n",
    "\n",
    "1) Set PHIMLIM very low (e.g., 1.0) and run the parameter estimation.  This throws away the soft-knowledge and finds the best fit to the observations (minimizes the measurement objective function).  \n",
    "\n",
    "2) Set PHIMLIM to something like __10% higher__ than this lowest Phi.  Re-run the parameter estimation, evaluate if the parameters are too extreme.  If they are, raise PHIMLIM again.\n",
    "\n",
    "We'll use this workflow on our pilot point version of Freyberg later.  But first, let's talk a little more about the theory and implementation of ``prior_information`` in the PEST datasets..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os, shutil\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import numpy as np\n",
    "from IPython.display import Image\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import flopy as flopy\n",
    "import pyemu\n",
    "import tsvd_helper as th\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import freyberg_setup as fs\n",
    "fs.setup_pest_pp()\n",
    "working_dir = fs.WORKING_DIR_PP\n",
    "pst_name = fs.PST_NAME_PP\n",
    "shutil.copy2(os.path.join(fs.BASE_MODEL_DIR,'hk.truth.ref'), os.path.join(working_dir,'hk.truth.ref'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tikhnov regularization is a special (and superior) kind of \"prior information\".  \n",
    "\n",
    "\n",
    "### In pyemu, we can add two forms of regularization:\n",
    "- preferred value: we want the parameter values to stay as close to the initial values as possible\n",
    "- preferred difference: we prefer the differences in parameter values to be minimized\n",
    "\n",
    "Preferred value is easy to understand, we simply add ``prior_information`` to the control file to enforce this condition.  pyemu uses a helper for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pre-constructed pst\n",
    "pst = pyemu.Pst(os.path.join(working_dir,pst_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the pyemu helper to apply preferred value regularization on all the parameters\n",
    "pyemu.helpers.zero_order_tikhonov(pst,parbounds=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a table of the regularization equations \n",
    "pst.prior_information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the \"regul\" appended to the parameter group name - that is how we tell PEST to track the deviations from preferred conditions separately as a Tikhonov regularization.\n",
    "\n",
    "Ok, that's fine, but should the weight on preferring a HK value be the same as preferring recharge not to change? HK is typically considered to be \"known\" within an order of magnitude; the uncertainty in recharge is typically considered less than that - say plus or minus 50%. Seems like we would want recharge to change less than HK. \n",
    "\n",
    "#### There is a neat trick that pyemu gives us: this strength of the preferred value can be inferred from the parameter bounds you specify.  That is, the bounds are used to form the regularization weights; larger bounds = more uncertainty = less weight given to  maintaining the initial value during the parameter estimation.  \n",
    "\n",
    "Let's try this again using the bounds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct preferred value reguarization equations and use the bounds to calculate the regularization weight\n",
    "pyemu.helpers.zero_order_tikhonov(pst,parbounds=True)\n",
    "# print out the regularization equations that were constructed\n",
    "pst.prior_information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are given more strength for keeping recharge near its initial value...good!\n",
    "\n",
    "### So what about preferred difference regularization?  \n",
    "\n",
    "Well pyemu can do that too.  Remember that ``Cov``ariance matrix we keep talking about? It expresses the spatial relationship between pilot points (implied by the variogram), so we use to setup these prior information equations.  First we need to make a geostatistical structure to encapsulate the spatial relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = pyemu.geostats.ExpVario(contribution=1.0,a=2500.0)\n",
    "gs = pyemu.geostats.GeoStruct(variograms=v,nugget=0.0)\n",
    "ax = gs.plot()\n",
    "ax.grid()\n",
    "ax.set_ylim(0,2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to know where the pilot points are.  We can get this from the pilot point template file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dataframe called df_pp using pyemu helper and the pilot point template file hkpp.dat.tpl\n",
    "df_pp = pyemu.pp_utils.pp_tpl_to_dataframe(os.path.join(working_dir,\"hkpp.dat.tpl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's build a covariance matrix from the geostatistical structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a covariance matrix called cov using pyemu's geostatistics capabilities\n",
    "cov = gs.covariance_matrix(df_pp.x,df_pp.y,df_pp.parnme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the pyemu helper to construct preferred difference regularization equations \n",
    "# using the covariance for regularization weight\n",
    "pyemu.helpers.first_order_pearson_tikhonov(pst,cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the new regularization equations out\n",
    "pst.prior_information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happened?  We replace the preferred value equations with a bunch of new equations.  These equations each include two parameter names and have different weights - can you guess what the weights are?  The weights are the pearson correlation coefficients (CC) between the pilot points (remember those from way back?).  These CC values are calculated from the covariance matrix, which is implied by the geostatistical structure...whew! For example, ``hk00`` is \"close\" to ``hk01``, so they have a high CC value (equation 1).  Just for fun, go back and change the \"a\" parameter in the variogram and see how it changes the CC values.\n",
    "\n",
    "### Handy hint:  you can use both preferred value and preferred difference regularization in the same PEST control file, and even on the same parameter!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Tikhonov regularization is a go-to tool, and we'll dedicate an entire notebook to applying Tikhonov to our overfit pilot point calibration.  But, before we go let's think of regularization in the broadest context....\n",
    "\n",
    "### Recall that regularization refers to any approach that makes an illposed/underdetermined parameter estimation problem solvable.  \n",
    "\n",
    "Therefore, when you manually reduce the number of parameters such as zones you are doing a type of regularization.  \n",
    "\n",
    "# So why not Truncated Singular Value Decomposition (TSVD) as a regularization device?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"PEST++V3_cover.jpeg\" style=\"float: left\">\n",
    "\n",
    "<img src=\"flopylogo.png\" style=\"float: right\">\n",
    "\n",
    "<img src=\"AW&H2015.png\" style=\"float: center\">\n",
    "\n",
    "\n",
    "\n",
    "Singular Value Decomposition can be thought of as operating in a similar fashion as you manually changing zones but is automated, and more clever.  Using the Jacobian matrix, it reduces the number of base parameters by making a reduced set of linear combinations of the base parameters.  Thus two perfectly correlated parameters become 1 combined parameter, which helps give a unique solution to the parameter estimation problem. Those linear combinations (__=singular values__) that are in the noise (__=null space__) get truncated (__=removed from the parameter estimation process__).  This means that insensitive parameters don't adversely affect the parameter estimation process.  Those linear combinations that remain (__=solution space__) are then used to solve the parameters estimation problem.  \n",
    "\n",
    "This truncated SVD approach makes for a parameter estimation process that is __*unconditionally stable*__, which means it is guaranteed to be well-posed, and solvable. __But the number of singular values also controls, in a somewhat brute force way, the degree of parameter smoothing and fit. The key to stability and degree of smoothing is this truncation process, which we'll dig into here.__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's see what TSVD does for our pilot point overfitting\n",
    "\n",
    "First, let's make the Jacobian matrix at the starting conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inpst = pyemu.Pst(os.path.join(working_dir,'freyberg_pp.pst'))\n",
    "# tell PEST that you only want a Jacobian matrix, which is NOPTMAX=-1\n",
    "inpst.control_data.noptmax=-1\n",
    "# have pyemu write out PEST files where the control file is called freyberg_jac.pst\n",
    "inpst.write(os.path.join(working_dir,'freyberg_jac.pst'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now start 15 workers to run the Jacobian\n",
    "(wait for it, wait for it...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('freyberg_pp')\n",
    "pyemu.helpers.start_slaves('.',\"pestpp\",'freyberg_jac.pst',num_slaves=15,master_dir='.')\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a Jacobian matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## now let's look at the singular value spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the  Jacobian and, form the Schur complement \n",
    "injac = pyemu.Schur(os.path.join(working_dir,'freyberg_jac.jcb'))\n",
    "# do the SVD linear algebra\n",
    "U,S,V = np.linalg.svd(injac.xtqx.df())\n",
    "# plot it up\n",
    "plt.bar(range(len(S)),S)\n",
    "plt.yscale('log')\n",
    "plt.grid('on')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Note the log scale on the y-axis.  Quite a spread! Recall the higher the bar the more information that supports the linear combination of base parameters in the singular value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So what if we only use superparameters comprising the first 2 singular vectors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the existing PEST control file and change some PEST++ options\n",
    "inpst = pyemu.Pst(os.path.join(working_dir,'freyberg_pp.pst'))\n",
    "# run a lot iterations so it finds an optimal parameter set\n",
    "inpst.pestpp_options['n_iter_super'] = 100\n",
    "# set the number of singular values to 2\n",
    "inpst.pestpp_options['max_n_super'] = 2\n",
    "# echo out the PEST++ options that we have so far\n",
    "inpst.pestpp_options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the new PEST control file \n",
    "inpst.write(os.path.join(working_dir,'freyberg_TSVD.pst'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now run a parameter estimation with 15 workers\n",
    "os.chdir('freyberg_pp')\n",
    "pyemu.helpers.start_slaves('.',\"pestpp\",'freyberg_TSVD.pst',num_slaves=15,master_dir='.')\n",
    "os.chdir('..')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make table that uses the PEST++ csv file output from the .iobj file\n",
    "indf = pd.read_csv(os.path.join(working_dir,'freyberg_TSVD.iobj'))\n",
    "# echo out the table we just defined\n",
    "indf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change directory\n",
    "os.chdir('freyberg_pp')\n",
    "# update the HK field with the new optimal values\n",
    "th.update_K('freyberg_TSVD.pst')\n",
    "# change directory\n",
    "os.chdir('..')\n",
    "# plot out results\n",
    "th.plot_K_results(working_dir, 'freyberg_TSVD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The left HK field is \"truth\", the other two are the optimal field with two different ranges for the color flood.  Looks smoother....and there still was a pretty big reduction in Phi....\n",
    "\n",
    "\n",
    "# How does this compare with the pilot points solution?\n",
    "\n",
    "Here's a jpeg of the previous results.  You can re-run the problem by executing the optional code blocks below, but be advised that it takes a while to run....\n",
    "\n",
    "<img src=\"Freyberg_overfit_pilot_point_run.jpeg\" style=\"float: center\">\n",
    "\n",
    "\n",
    "### Yes the fields with TSVD = 2 singular values is much smoother than the original parameter estimation with no regularization.  So, TSVD can be a regularization device!\n",
    "\n",
    "\n",
    "# OPTIONAL CODE to rerun pilot point version\n",
    "\n",
    "(and recreate the jpeg figure above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inpst = pyemu.Pst(os.path.join(working_dir,'freyberg_pp.pst'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('freyberg_pp')\n",
    "pyemu.helpers.start_slaves('.',\"pestpp\",'freyberg_pp.pst',num_slaves=15,master_dir='.')\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('freyberg_pp')\n",
    "th.update_K('freyberg_pp.pst')\n",
    "os.chdir('..')\n",
    "pst.phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th.plot_K_results(working_dir, 'freyberg_pp')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
