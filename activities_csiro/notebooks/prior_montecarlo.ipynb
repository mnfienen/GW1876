{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run and process the prior monte carlo and pick a \"truth\" realization\n",
    "\n",
    "A great advantage of exploring a synthetic model is that we can enforce a \"truth\" and then evaluate how our various attempts to estimate it perform. One way to do this is to run a monte carlo ensemble of multiple parameter realizations and then choose one of them to represent the \"truth\". That will be accomplished in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "plt.rcParams['font.size']=12\n",
    "import flopy\n",
    "import pyemu\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SUPER IMPORTANT: SET HOW MANY PARALLEL WORKERS TO USE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set the `t_d` or \"template directory\" variable to point at the template folder and read in the PEST control file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_d = \"template\"\n",
    "pst = pyemu.Pst(os.path.join(t_d,\"freyberg.pst\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decide what pars are uncertain in the truth\n",
    "\n",
    "We need to decide what our truth looks like - should the pilot points or the grid-scale pars be the source of spatial variability? or both?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par = pst.parameter_data\n",
    "# grid pars\n",
    "#should_fix = par.loc[par.pargp.apply(lambda x: \"gr\" in x),\"parnme\"]\n",
    "# pp pars\n",
    "#should_fix = par.loc[par.pargp.apply(lambda x: \"pp\" in x),\"parnme\"]\n",
    "#pst.npar - should_fix.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe = pyemu.ParameterEnsemble.from_binary(pst=pst,filename=os.path.join(t_d,\"prior.jcb\"))\n",
    "#pe.loc[:,should_fix] = 1.0\n",
    "pe.to_csv(os.path.join(t_d,\"sweep_in.csv\"))\n",
    "pe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe.loc[:,\"hk031\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe.loc[:,\"hk031\"].plot.hist(bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "look! hk is log-normal-ish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.parameter_data.loc[pe.columns,\"parval1\"] = pe.iloc[0,:]\n",
    "pst.control_data.noptmax = 0\n",
    "pst.write(os.path.join(t_d,\"test.pst\"))\n",
    "pyemu.os_utils.run(\"pestpp-ies test.pst\",cwd=t_d)\n",
    "res = pyemu.pst_utils.read_resfile(os.path.join(t_d,\"test.base.rei\"))\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run the prior ensemble in parallel locally\n",
    "This takes advantage of the program `pestpp-swp` which runs a parameter sweep through a set of parameters. By default, `pestpp-swp` reads in the ensemble from a file called `sweep_in.csv` which in this case we made just above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_d = \"master_prior_sweep\"\n",
    "pyemu.os_utils.start_slaves(t_d,\"pestpp-swp\",\"freyberg.pst\",num_slaves=num_workers,slave_root=\".\",master_dir=m_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the output ensemble and plot a few things\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_df = pd.read_csv(os.path.join(m_d,\"sweep_out.csv\"),index_col=0)\n",
    "print('number of realization in the ensemble before dropping: ' + str(obs_df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### drop any failed runs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_df = obs_df.loc[obs_df.failed_flag==0,:]\n",
    "print('number of realization in the ensemble **after** dropping: ' + str(obs_df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_df.iloc[0,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### confirm which quantities were identified as forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames = pst.pestpp_options[\"forecasts\"].split(',')\n",
    "fnames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### now we can plot the distributions of each forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for forecast in fnames:\n",
    "    plt.figure()\n",
    "    ax = obs_df.loc[:,forecast].plot(kind=\"hist\")\n",
    "    ax.set_title(forecast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that under scenario conditions, many more realizations for the flow to the aquifer in the headwaters are postive (as expected).  Lets difference these two:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfnames = [f for f in fnames if \"1980\" in f or \"_001\" in f]\n",
    "hfnames = [f for f in fnames if \"1979\" in f or \"_000\" in f]\n",
    "diff = obs_df.loc[:,hfnames].values - obs_df.loc[:,sfnames].values\n",
    "diff = pd.DataFrame(diff,columns=sfnames)\n",
    "diff.hist(figsize=(10,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now see that the most extreme scenario yields a large decrease in flow from the aquifer to the headwaters (the most negative value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setting the \"truth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just need to replace the observed values (`obsval`) in the control file with the outputs for one of the realizations on `obs_df`.  In this way, we now have the nonzero values for history matching, but also the ``truth`` values for comparing how we are doing with other unobserved quantities.  I'm going to pick a realization that yields an \"average\" variability of the observed gw levels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_vals = obs_df.loc[:,\"part_time\"].sort_values()\n",
    "idx = sorted_vals.index[100]\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_df.loc[idx,pst.nnz_obs_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see how our selected truth does with the sw/gw forecasts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_df.loc[idx,fnames]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign some initial weights. Now, it is custom to add noise to the observed values...we will use the classic Gaussian noise...zero mean and standard deviation of 1 over the weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst = pyemu.Pst(os.path.join(t_d,\"freyberg.pst\"))\n",
    "obs = pst.observation_data\n",
    "obs.loc[:,\"obsval\"] = obs_df.loc[idx,pst.obs_names]\n",
    "obs.loc[obs.obgnme==\"calhead\",\"weight\"] = 10.0\n",
    "obs.loc[obs.obgnme==\"calflux\",\"weight\"] = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we just get a sample from a random normal distribution with mean=0 and std=1.\n",
    "The argument indicates how many samples we want - and we choose `pst.nnz_obs` which is the \n",
    "the number of nonzero-weighted observations in the PST file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed=0)\n",
    "snd = np.random.randn(pst.nnz_obs)\n",
    "noise = snd * 1./obs.loc[pst.nnz_obs_names,\"weight\"]\n",
    "pst.observation_data.loc[noise.index,\"obsval\"] += noise\n",
    "noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we write this out to a new file and run `pestpp-ies` to see how the objective function looks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.write(os.path.join(t_d,\"freyberg.pst\"))\n",
    "pyemu.os_utils.run(\"pestpp-ies freyberg.pst\",cwd=t_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can read in the results and make some figures showing residuals and the balance of the objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst = pyemu.Pst(os.path.join(t_d,\"freyberg.pst\"))\n",
    "print(pst.phi)\n",
    "plt.figure()\n",
    "pst.plot(kind='phi_pie');\n",
    "print('Here are the non-zero weighted observation names')\n",
    "\n",
    "figs = pst.plot(kind=\"1to1\");\n",
    "pst.res.loc[pst.nnz_obs_names,:]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the truth you chose, we may have a problem - we set the weights for both the heads and the flux to reasonable values based on what we expect for measurement noise. But the contributions to total phi might be out of balance - if contribution of the flux measurement to total phi is too low, the history matching excersizes (coming soon!) will focus almost entirely on minimizing head residuals.  So we need to balance the objective function.  This is a subtle but very important step, especially since some of our forecasts deal with sw-gw exchange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = pst.phi_components\n",
    "target = {\"calflux\":0.3 * pc[\"calhead\"]}\n",
    "#target = {\"calhead\":500,\"calflux\":500}\n",
    "pst.adjust_weights(obsgrp_dict=target)\n",
    "pst.plot(kind='phi_pie')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see what the new flux observation weight is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.observation_data.loc[pst.nnz_obs_names,\"weight\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for some super trickery: since we changed the weight, we need to generate the observation noise using these new weights for the error model (so meta!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = pst.observation_data\n",
    "np.random.seed(seed=0)\n",
    "snd = np.random.randn(pst.nnz_obs)\n",
    "noise = snd * 1./obs.loc[pst.nnz_obs_names,\"weight\"]\n",
    "obs.loc[:,\"obsval\"] = obs_df.loc[idx,pst.obs_names]\n",
    "pst.observation_data.loc[noise.index,\"obsval\"] += noise\n",
    "noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.write(os.path.join(t_d,\"freyberg.pst\"))\n",
    "pyemu.os_utils.run(\"pestpp-ies freyberg.pst\",cwd=t_d)\n",
    "pst = pyemu.Pst(os.path.join(t_d,\"freyberg.pst\"))\n",
    "print(pst.phi)\n",
    "pst.plot(kind='phi_pie')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whew!  confused yet?  Ok, let's leave all this confusion behind...its mostly academic, just to make sure we are using weights that are in harmony with the noise we added to the truth...Just to make sure we have everything working right, we should be able to load the truth parameters, run the model once and have a `phi` equivalent to the noise vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par_df = pd.read_csv(os.path.join(m_d,\"sweep_in.csv\"),index_col=0)\n",
    "pst.parameter_data.loc[:,\"parval1\"] = par_df.loc[idx,pst.par_names]\n",
    "pst.write(os.path.join(m_d,\"test.pst\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will run this with `noptmax=0` to preform a single run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyemu.os_utils.run(\"pestpp-ies.exe test.pst\",cwd=m_d)\n",
    "pst = pyemu.Pst(os.path.join(m_d,\"test.pst\"))\n",
    "print(pst.phi)\n",
    "pst.res.loc[pst.nnz_obs_names,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The residual should be exactly the noise values from above. Lets load the model (that was just run using the true pars) and check some things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = flopy.modflow.Modflow.load(\"freyberg.nam\",model_ws=m_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = m.upw.hk[0].array\n",
    "#a = m.rch.rech[0].array\n",
    "a = np.ma.masked_where(m.bas6.ibound[0].array==0,a)\n",
    "print(a.min(),a.max())\n",
    "c = plt.imshow(a)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = flopy.utils.MfListBudget(os.path.join(m_d,\"freyberg.list\"))\n",
    "df = lst.get_dataframes(diff=True)[0]\n",
    "ax = df.plot(kind=\"bar\",figsize=(10,10), grid=True)\n",
    "a = ax.set_xticklabels([\"historic\",\"scenario\"],rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### see how our existing observation ensemble compares to the truth\n",
    "\n",
    "forecasts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = pst.observation_data\n",
    "plt.figure()\n",
    "for forecast in fnames:\n",
    "    ax = plt.subplot(111)\n",
    "    obs_df.loc[:,forecast].hist(ax=ax,color=\"0.5\",alpha=0.5)\n",
    "    ax.plot([obs.loc[forecast,\"obsval\"],obs.loc[forecast,\"obsval\"]],ax.get_ylim(),\"r\")\n",
    "    ax.set_title(forecast)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for oname in pst.nnz_obs_names:\n",
    "    ax = plt.subplot(111)\n",
    "    obs_df.loc[:,oname].hist(ax=ax,color=\"0.5\",alpha=0.5)\n",
    "    ax.plot([obs.loc[oname,\"obsval\"],obs.loc[oname,\"obsval\"]],ax.get_ylim(),\"r\")\n",
    "    ax.set_title(oname)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
