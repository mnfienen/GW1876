{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run PESTPP-OPT\n",
    "\n",
    "In this notebook we will setup and solve a mgmt optimization problem around how much groundwater can be pumped while maintaining sw-gw exchange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "plt.rcParams['font.size']=12\n",
    "import flopy\n",
    "import pyemu\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SUPER IMPORTANT: SET HOW MANY PARALLEL WORKERS TO USE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_d = \"template_history\"\n",
    "m_d = \"master_opt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can look at the summary information about the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst = pyemu.Pst(os.path.join(t_d,\"freyberg.pst\"))\n",
    "pst.write_par_summary_table(filename=\"none\").sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define our decision variable group and also set some `++args`.\n",
    "\n",
    "Conceptually, we are going to optimize current pumping rates to make sure we meet ecological flows under both historic (current) conditions and scenario (future) conditions.  Remember the scenario is an extreme 1-year drought, so if we pump too much now, the system will be too low to provide critical flows if next year is an extreme drough - transient memory!\n",
    "\n",
    "Define a parameter group as the devision variables (i.e. the variables that we will tune to meet the optimal condition). We will define `wellflux_k02` as the decision variable group (defined by the `++arg` called `opt_dec_var_groups`. Note in the table above that this group represents (time-invariant) flux mulipliers for each of the 6 wells (we will however only optimize for pumping rates in current (historic) conditions). \n",
    "\n",
    "We can also define which direction we want the optimization to go using `opt_direction` as `max`. This means the objective of the optimization will be to maximize future pumping subject to the constraints we will establish below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.pestpp_options = {}\n",
    "#dvg = [\"welflux_k02\",\"welflux\"]\n",
    "dvg = [\"welflux_k02\"]  # time-invariant flux multiplier for each well\n",
    "pst.pestpp_options[\"opt_dec_var_groups\"] = dvg\n",
    "pst.pestpp_options[\"opt_direction\"] = \"max\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first run, we won't use chance constraints, so just fix all non-decision-variable \"parameters\".  We also need to set some realistic bounds on the `welflux` multiplier decision variables.  Finally, we need to specify a larger derivative increment for the decision varible group. For typical parameter estimation, `derinc=0.01` is often sufficient for calculating a Jacobian matrix. But, for the response matrix method of optimization, the response can be subtle requiring a greater perturbation increment. We will set it to `0.25` using some `pandas` manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par = pst.parameter_data\n",
    "par.loc[:,\"partrans\"] = \"fixed\"\n",
    "\n",
    "temporal_wel_pars = par.loc[par.parnme.apply(lambda x: \"welflux\" in x),\"parnme\"]\n",
    "#print(temporal_wel_pars)\n",
    "#par.loc[temporal_wel_pars,\"parlbnd\"] = 0.0 \n",
    "par.loc[temporal_wel_pars,\"parval1\"] = 1.0\n",
    "\n",
    "# # dec vars\n",
    "dvg_pars = par.loc[par.pargp.apply(lambda x: x in dvg),\"parnme\"]\n",
    "par.loc[dvg_pars,\"partrans\"] = \"none\"\n",
    "par.loc[dvg_pars,\"parlbnd\"] = 0.0\n",
    "par.loc[dvg_pars,\"parubnd\"] = 3.0  # corresponds to -450 m3/d\n",
    "par.loc[dvg_pars,\"parval1\"] = 1.0\n",
    "\n",
    "par.loc[dvg_pars,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.rectify_pgroups()\n",
    "pst.parameter_groups.loc[dvg,\"inctyp\"] = \"absolute\"\n",
    "pst.parameter_groups.loc[dvg,\"inctyp\"] = \"absolute\"\n",
    "pst.parameter_groups.loc[dvg,\"derinc\"] = 0.25\n",
    "\n",
    "pst.parameter_groups.loc[dvg,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define constraints\n",
    "\n",
    "Model-based or dec var-related constraints are identified in `pestpp-opt` by an obs or prior information group that starts with \"less_than\" or \"greater_than\" and a weight greater than zero.  So first, we turn off all of the weights and get names for the sw-gw exchange forecasts (funny how optimization turns forecasts into constraints...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = pst.observation_data\n",
    "obs.loc[:,\"weight\"] = 0.0\n",
    "swgw_const = obs.loc[obs.obsnme.apply(lambda x: \"fa\" in x and (\"hw\" in x or \"tw\" in x)),\"obsnme\"]\n",
    "obs.loc[swgw_const,:].obsval.hist(bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to change the obs group (`obgnme`) so that `pestpp-opt` will recognize these model outputs as constraints.  The `obsval` becomes the RHS of the constraint.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs.loc[swgw_const,\"obgnme\"] = \"less_than\"\n",
    "obs.loc[swgw_const,\"weight\"] = 1.0\n",
    "\n",
    "# we must have at least 300 m3/day of flux from gw to sw\n",
    "# for historic and scenario periods\n",
    "# and for both headwaters and tailwaters\n",
    "obs.loc[swgw_const,\"obsval\"] = -100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to define a minimum total pumping rate, otherwise this opt problem might yield a solution that doesn't give enough water for the intended usage.  We will do this through a prior information constraint since this just a sum of decision variable values - the required minimum value will the sum of current pumping rates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyemu.pst_utils.pst_config[\"prior_fieldnames\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since all pumping wells are using the same rate and are of same \"water supply benefit\", we can just use a `1.0` multiplier in front of each `wel.flux` decision varialbe.  If that is not the case, then you need to set the multipliers to be more meaningful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = pst.null_prior\n",
    "pi.loc[\"pi_1\",\"obgnme\"] = \"greater_than\"\n",
    "pi.loc[\"pi_1\",\"pilbl\"] = \"pi_1\"\n",
    "pi.loc[\"pi_1\",\"equation\"] = \" + \".join([\"1.0 * {0}\".format(d) for d in dvg_pars]) +\\\n",
    "                            \" = {0}\".format(par.loc[dvg_pars,\"parval1\"].sum())\n",
    "pi.loc[\"pi_1\",\"weight\"] = 1.0\n",
    "pi.equation[\"pi_1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.prior_information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## now for a risk-neutral optimization (ignoring uncertainty in constraints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note that setting `noptmax=1` is equivalent to selecting Linear Programming (LP) as the optimization algorithm (thus assuming a linear response matrix).\n",
    "\n",
    "#### A higher value of `noptmax` runs Sequential Linear Programming (SLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.control_data.noptmax = 1\n",
    "pst.write(os.path.join(t_d,\"freyberg_opt.pst\"))\n",
    "pyemu.os_utils.start_workers(t_d,\"pestpp-opt\",\"freyberg_opt.pst\",num_workers=num_workers,master_dir=m_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load and inspect the response matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jco = pyemu.Jco.from_binary(os.path.join(m_d,\"freyberg_opt.1.jcb\")).to_dataframe().loc[pst.less_than_obs_constraints,:]\n",
    "jco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the transient effects in the nonzero value between current pumping rates (columns) and scenario sw-gw exchange (rows from 1980) and that the current 1979 sw-gw exchanges are always larger than those in the scenario 1980 condition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also load the optimal decision variable values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par_df = pyemu.pst_utils.read_parfile(os.path.join(m_d,\"freyberg_opt.1.par\"))\n",
    "print(par_df.loc[dvg_pars,\"parval1\"].sum())\n",
    "par_df.loc[dvg_pars,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sum of these values is the optimal objective function value. However, since these are just mulitpliers on the pumping rate, this number isnt too meaningful. Instead, lets look at the residuals file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst = pyemu.Pst(os.path.join(m_d,\"freyberg_opt.pst\"),resfile=os.path.join(m_d,\"freyberg_opt.1.sim.rei\"))\n",
    "pst.res.loc[pst.nnz_obs_names,:].sort_values(by=\"modelled\",ascending=False).modelled.iloc[:10].plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see the \"binding constraints\" as those at -100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We better also check that our prior information constraint (min total abstracted water allocation) is being met.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.res.loc[pst.prior_names,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opt under uncertainty part 1: FOSM chance constraints\n",
    "\n",
    "This is where the process of uncertainty quantification/history matching and mgmt optimizatiom meet - worlds collide! \n",
    "\n",
    "Mechanically, in PESTPP-OPT, to activate the chance constraint process, we need to specify a `risk != 0.5`.  Risk ranges from 0.001 (risk tolerant) to 0.999 (risk averse).  The larger the risk value, the more confidence we have that the (uncertain) model-based constraints are truely satisfied.  Here we will start with a risk tolerant stance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.pestpp_options[\"opt_risk\"] = 0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the FOSM-based chance constraints, we also need to have at least one adjustable non-dec-var parameter so that we can propogate parameter uncertainty to model-based constraints (this can also be posterior FOSM is non-constraint, non-zero-weight observations are specified).  For this simple demo, lets just use the constant multiplier parameters in the prior uncertainty stance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cn_pars = par.loc[par.pargp.apply(lambda x: \"cn\" in x),\"parnme\"]\n",
    "cn_pars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par = pst.parameter_data\n",
    "par.loc[cn_pars,\"partrans\"] = \"log\"\n",
    "pst.control_data.noptmax = 1\n",
    "pst.write(os.path.join(t_d,\"freyberg_opt_uu1.pst\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we need to not only fill the response matrix (between dec vars and constraints) but we also need to fill the jacobian matrix (between parameters and constraints).  Given that we only have 6 decision variables, let's just re-populate the response matrix while also populating the Jacobian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyemu.os_utils.start_workers(t_d,\"pestpp-opt\",\"freyberg_opt_uu1.pst\",num_workers=num_workers,master_dir=m_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do we expect to see here?  What should happen to the optimal dec vars? And the constraints?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst = pyemu.Pst(os.path.join(m_d,\"freyberg_opt_uu1.pst\"),resfile=os.path.join(m_d,\"freyberg_opt_uu1.1.sim.rei\"))\n",
    "pst.res.loc[pst.nnz_obs_names,:].sort_values(by=\"modelled\",ascending=False).modelled.iloc[:20].plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait - was the constraint that sw-gw flux had to be less than -100?  How do we have some many constraints greater than -100???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par_df = pyemu.pst_utils.read_parfile(os.path.join(m_d,\"freyberg_opt_uu1.1.par\"))\n",
    "print(par_df.loc[dvg_pars,\"parval1\"].sum())\n",
    "par_df.loc[dvg_pars,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now see how taking a risk tolerant stance allows for more pumping but that we have only a 40% chance of actually satifying the sw-gw constraints (see how the model simulated value is actually in violation of the -100 constraint RHS).  Lets check the residuals that include the FOSM-based chance constraint shift:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df = pyemu.pst_utils.read_resfile(os.path.join(m_d,\"freyberg_opt_uu1.1.sim+fosm.rei\")).loc[pst.nnz_obs_names,:]\n",
    "res_df.sort_values(by=\"modelled\",ascending=False).modelled.iloc[:10].plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See what is happening?  Because we were risk tolerant, we actually subtracted some sw-gw flux from the values from MODFLOW - effectively acounting for additional sw-gw that wasnt acutally simulated.  This flux is in the uncertainty in the simulated sw-gw flux.  And since we were \"aggresive\" with our risk-stance, we can exploit this uncertainty to extract additional groundwater."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opt under uncertainty part 2: ensemble-based chance constraints\n",
    "\n",
    "PESTPP-OPT can also skip the FOSM calculations if users specify model-based constraint weights as standard deviations (i.e. uncertainty in the forecasts/constraints).  These can be derived from our existing ensembles (oh snap!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_df = pd.read_csv(os.path.join(\"master_prior_sweep\",\"sweep_out.csv\"),index_col=0)\n",
    "obs_df = obs_df.loc[obs_df.failed_flag==0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_std = obs_df.std().loc[pst.nnz_obs_names]\n",
    "pr_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note we can also skip the FOSM calcs within `PESTPP-OPT` using weights as per previous FOSM standard deviation calcs, not just ensemble-based standard deviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.observation_data.loc[pst.nnz_obs_names,\"weight\"] = pr_std.loc[pst.nnz_obs_names]\n",
    "pst.pestpp_options[\"opt_std_weights\"] = True\n",
    "pst.write(os.path.join(t_d,\"freyberg_opt_uu2.pst\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyemu.os_utils.start_workers(t_d,\"pestpp-opt\",\"freyberg_opt_uu2.pst\",num_workers=num_workers,master_dir=m_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par_df = pyemu.pst_utils.read_parfile(os.path.join(m_d,\"freyberg_opt_uu2.1.par\"))\n",
    "print(par_df.loc[dvg_pars,\"parval1\"].sum())\n",
    "par_df.loc[dvg_pars,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is the objective function higher when we use the ensemble-based constraint uncertainty compared to the FOSM constraint uncertainty?  Remember how many more parameters were used in the ensemble analyses compared to just the hand full of constant by layer parameters that we used for the FOSM calcs within PESTPP-OPT?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Super secret mode for `LP`\n",
    "\n",
    "It turns out, if the opt problem is truely linear, we can reuse results of a previous PESTPP-OPT run to modify lots of the pieces of the optimization problem and resolve the optimization problem without running the model even once!  WAT!? \n",
    "\n",
    "As long as the same decision variables are relates to the same responses, and we can fairly assume that the response matrix that relates the decision variables to the constraints is linear, then the response matrix doesn't change even if things like bounds and risk level change. We just need `pestpp-opt` to read in the response matrix (which is stored with the same format as a Jacobian (`jcb`)) and the residuals (`rei`). \n",
    "\n",
    "This is done by specifying some additional `++args` (and copying some files around)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.copy2(os.path.join(m_d,\"freyberg_opt_uu2.1.jcb\"),os.path.join(m_d,\"restart.jcb\"))\n",
    "shutil.copy2(os.path.join(m_d,\"freyberg_opt_uu2.1.jcb.rei\"),os.path.join(m_d,\"restart.rei\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have copied over the necessary files, we set a few `++args`:  \n",
    "* `base_jacobian`: this instructs `pestpp-opt` to read in the existing response matrix\n",
    "* `hotstart_resfile`: this instructs `pestpp-opt` to use the residuals we already have\n",
    "* `opt_skip_final`: this waives the usual practice of running the model once with optimal parameter values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which runs do each of these skip specifically?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.pestpp_options[\"base_jacobian\"] = \"restart.jcb\"\n",
    "pst.pestpp_options[\"hotstart_resfile\"] = \"restart.rei\"\n",
    "pst.pestpp_options[\"opt_skip_final\"] = True\n",
    "pst.write(os.path.join(m_d,\"freyberg_opt_restart.pst\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyemu.os_utils.run(\"pestpp-opt freyberg_opt_restart.pst\",cwd=m_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par_df = pyemu.pst_utils.read_parfile(os.path.join(m_d,\"freyberg_opt_restart.1.par\"))\n",
    "print(par_df.loc[dvg_pars,\"parval1\"].sum())\n",
    "par_df.loc[dvg_pars,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh snap!  that means we can do all sort of kewl optimization testing really really fast..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## now let's try taking a risk averse stance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk = 0.55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.pestpp_options[\"opt_risk\"] = risk\n",
    "pst.write(os.path.join(m_d,\"freyberg_opt_restart.pst\"))\n",
    "pyemu.os_utils.run(\"pestpp-opt freyberg_opt_restart.pst\",cwd=m_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par_df = pyemu.pst_utils.read_parfile(os.path.join(m_d,\"freyberg_opt_restart.1.par\"))\n",
    "print(par_df.loc[dvg_pars,\"parval1\"].sum())\n",
    "par_df.loc[dvg_pars,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### now lets evaluate how our OUU problem changes if we use posterior standard deviations - this is a critically important use of the uncertainty analysis from history matching...\n",
    "\n",
    "This is where all that painful history-matching finally has a real-world (and valid) purpose: to reduce the uncertainty in the constraints for a decision-support optimization problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_df = pd.read_csv(os.path.join(\"master_ies\",\"freyberg_ies.3.obs.csv\"),index_col=0)\n",
    "\n",
    "#df = df=pd.read_csv(os.path.join(\"master_glm\",\"freyberg_pp.post.obsen.csv\"),index_col=0)\n",
    "#obs_df = pyemu.ObservationEnsemble.from_dataframe(pst=pst,df=df)\n",
    "#obs_df = obs_df.loc[obs_df.phi_vector.sort_values().index[:20],:] \n",
    "pt_std = obs_df.std().loc[pst.nnz_obs_names]\n",
    "obs_df.std().loc[pst.nnz_obs_names]\n",
    "#obs_df.max().loc[pst.nnz_obs_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much lower is the posterior standard deviations as compared to the prior?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\"prior\":pr_std,\"posterior\":pt_std}).plot(kind=\"bar\",figsize=(20,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implies that the chance constraints (which express the important model input uncertainty propogated to the forecast/constraints) is significantly lower, meaning uncertainty has less \"value\" in the optimization objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.observation_data.loc[pst.nnz_obs_names,\"weight\"] = pt_std.loc[pst.nnz_obs_names]\n",
    "pst.observation_data.loc[pst.nnz_obs_names,\"weight\"].plot(kind=\"hist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.write(os.path.join(m_d,\"freyberg_opt_restart.pst\"))\n",
    "pyemu.os_utils.run(\"pestpp-opt freyberg_opt_restart.pst\",cwd=m_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par_df = pyemu.pst_utils.read_parfile(os.path.join(m_d,\"freyberg_opt_restart.1.par\"))\n",
    "print(par_df.loc[dvg_pars,\"parval1\"].sum())\n",
    "par_df.loc[dvg_pars,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pyemu.pst_utils.read_resfile(os.path.join(m_d,\"freyberg_opt_restart.1.est+fosm.rei\")).loc[pst.nnz_obs_names,:]\n",
    "df.sort_values(by=\"modelled\",ascending=False).modelled.iloc[:10].plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we see that historic tail water flux is the binding constraint.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## some reformulation of the opt problem. Can we open up decision variable (feasibility) space?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets reformulate the problem to be constrained by the total sw-gw flux across all reaches instead of splitting into headwaters and tailwaters.  Good thing we have added the list file budget components to the control file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst = pyemu.Pst(os.path.join(m_d,\"freyberg_opt_restart.pst\"))\n",
    "obs = pst.observation_data\n",
    "obs.loc[pst.nnz_obs_names,\"obgnme\"] = \"sw-gw\"  # hw and tw constraints from tagged \"less_than\"\n",
    "obs.loc[pst.nnz_obs_names,\"weight\"] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_swgw = obs.loc[obs.obsnme.str.startswith(\"flx_stream_\"),\"obsnme\"]\n",
    "tot_swgw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs.loc[tot_swgw,\"obgnme\"] = \"less_than\"\n",
    "obs.loc[tot_swgw,\"weight\"] = 1.0\n",
    "obs.loc[tot_swgw,\"weight\"] = obs_df.std().loc[pst.nnz_obs_names]\n",
    "obs.loc[tot_swgw,\"obsval\"] = -200  # simple linear sum\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check your understanding:  we just reformulated the mgmt optimization problem to use the total sw-gw flux along all reaches instead of dividing the constraints into a headwater and tailwater reach grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_df.std().loc[pst.nnz_obs_names].plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## risk sweeps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we really want to find the most risk averse stance that is still feasible we will run a sweep of risk values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par_dfs = []\n",
    "res_dfs = []\n",
    "risk_vals = np.arange(0.05,1.0,0.05)\n",
    "for risk in risk_vals:\n",
    "    #try:\n",
    "    #    os.remove(os.path.join(m_d,\"freyberg_opt_restart.1.est+fosm.rei\"))\n",
    "    #except:\n",
    "    #    pass\n",
    "   \n",
    "    pst.pestpp_options[\"opt_risk\"] = risk\n",
    "    pst.pestpp_options[\"opt_skip_final\"] = True\n",
    "    print(\"undertaking evaluation for risk value: {0:.2f}\".format(risk))\n",
    "    pst.write(os.path.join(m_d,\"freyberg_opt_restart.pst\"))\n",
    "    pyemu.os_utils.run(\"pestpp-opt freyberg_opt_restart.pst\",cwd=m_d)\n",
    "    \n",
    "    par_df = pyemu.pst_utils.read_parfile(os.path.join(m_d,\"freyberg_opt_restart.1.par\"))\n",
    "    par_df = par_df.loc[dvg_pars,:]\n",
    "    #when the solution is infeasible, pestpp-opt writes extreme negative values \n",
    "    # to the par file:\n",
    "    if par_df.parval1.sum() < 6.0: \n",
    "        print(\"infeasible at risk {0:.2f}\".format(risk))\n",
    "        break\n",
    "    \n",
    "    res_df = pyemu.pst_utils.read_resfile(os.path.join(m_d,\"freyberg_opt_restart.1.est+fosm.rei\"))\n",
    "    res_df = res_df.loc[pst.nnz_obs_names,:]\n",
    "    res_dfs.append(res_df.modelled)\n",
    "    par_dfs.append(par_df.parval1)\n",
    "\n",
    "# process the dec var and constraint dataframes for plotting\n",
    "risk_vals = risk_vals[:len(par_dfs)]\n",
    "par_df = pd.concat(par_dfs,axis=1).T\n",
    "par_df.index = risk_vals\n",
    "par_df.index = par_df.index.map(lambda x: \"{0:0.3f}\".format(x))\n",
    "res_df = pd.concat(res_dfs,axis=1).T\n",
    "res_df.index = risk_vals\n",
    "res_df.index = res_df.index.map(lambda x: \"{0:0.3f}\".format(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\"m\",\"c\",\"g\",\"r\",\"b\",\"orange\"]\n",
    "fig, axes = plt.subplots(2,1,figsize=(20,15))\n",
    "par_df.plot(kind=\"bar\",ax=axes[0],alpha=0.75,color=colors).legend(bbox_to_anchor=(1.2, 0.5))\n",
    "axes[0].set_ylabel(\"individual pumping rates\")\n",
    "axes[0].set_xticklabels([])\n",
    "res_df.plot(kind=\"bar\",ax=axes[1],alpha=0.75,edgecolor=\"none\").legend(bbox_to_anchor=(1.2, 0.5))\n",
    "axes[1].plot(axes[1].get_xlim(),[-200,-200],\"r--\",lw=3)\n",
    "axes[1].set_ylabel(\"sw-gw flux\")\n",
    "axes[1].set_xlabel(\"risk\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for some maps of pumping regimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = flopy.modflow.Modflow.load(\"freyberg.nam\",model_ws=t_d)\n",
    "\n",
    "wf_par = pst.parameter_data.loc[dvg_pars,:].copy()\n",
    "wf_par.loc[:,\"k\"] = wf_par.parnme.apply(lambda x: int(x[2:4]))\n",
    "wf_par.loc[:,\"i\"] = wf_par.parnme.apply(lambda x: int(x[4:8]))\n",
    "wf_par.loc[:,\"j\"] = wf_par.parnme.apply(lambda x: int(x[8:]))\n",
    "wf_par.loc[:,\"x\"] = wf_par.apply(lambda x: m.sr.xcentergrid[x.i,x.j],axis=1)\n",
    "wf_par.loc[:,\"y\"] = wf_par.apply(lambda x: m.sr.ycentergrid[x.i,x.j],axis=1)\n",
    "\n",
    "ib = m.bas6.ibound[0].array\n",
    "ib = np.ma.masked_where(ib!=0,ib)\n",
    "fig,axes = plt.subplots(5,int(np.ceil(par_df.shape[0]/5)),figsize=(12,12))\n",
    "axes = axes.flatten()\n",
    "for risk,ax in zip(par_df.index,axes):\n",
    "    ax.set_aspect(\"equal\")\n",
    "    #ax = plt.subplot(111,aspect=\"equal\") \n",
    "    ax.imshow(ib,extent=m.sr.get_extent())\n",
    "    ax.scatter(wf_par.x,wf_par.y,s=par_df.loc[risk,wf_par.parnme].values*50,c=colors)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_title(risk)\n",
    "    \n",
    "for i in range(par_df.shape[0],axes.shape[0]):\n",
    "    ax = axes[i]\n",
    "    ax.axis(\"off\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How slick was that!  no more model runs needed and yet we transformed the OUU problem (by swapping constraints) and solved for a much more risk averse stance!  Just to make sure, lets run the model with the most risk-averse decision variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.pestpp_options[\"opt_risk\"] = risk_vals[-1]\n",
    "pst.pestpp_options[\"opt_skip_final\"] = False\n",
    "pst.write(os.path.join(m_d,\"freyberg_opt_restart.pst\"))\n",
    "pyemu.os_utils.run(\"pestpp-opt freyberg_opt_restart.pst\",cwd=m_d)\n",
    "# load the simulated outputs plus the FOSM chance constraint offsets:\n",
    "res_df = pyemu.pst_utils.read_resfile(os.path.join(m_d,\"freyberg_opt_restart.1.sim+fosm.rei\"))\n",
    "res_df = res_df.loc[pst.nnz_obs_names,:]\n",
    "res_df.sort_values(by=\"modelled\",ascending=False).modelled.iloc[:10].plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the actual model simulated outputs\n",
    "res_df_sim = pyemu.pst_utils.read_resfile(os.path.join(m_d,\"freyberg_opt_restart.1.sim.rei\"))\n",
    "res_df_sim = res_df_sim.loc[pst.nnz_obs_names,:]\n",
    "ax = pd.DataFrame({\"sim\":res_df_sim.modelled,\"sim+fosm\":res_df.modelled}).plot(kind=\"bar\",figsize=(20,10))\n",
    "ax.plot(ax.get_xlim(),[-200,-200],\"r--\",lw=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see the cost of uncertainty - we have to simulate a greater flux from gw to sw to make sure (e.g. be risk averse) that the flux from  gw to sw is actually at least 200 m3/day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# FINALLY!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now see the reason for high-dimensional uncertainty quantification and history matching: to define and then reduce (through data assimilation) the uncertainty in the model-based constraints (e.g. sw-gw forecasts) so that we can find a more risk-averse management solution - we can use a model to identify an optimal pumping scheme to provide the volume of water needed for water supply, agriculture, etc. but also provide assurances (at the given confidence) that ecological flows will be maintained under both current conditions and in the event of an extreme 1-year drought.  BOOM!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
